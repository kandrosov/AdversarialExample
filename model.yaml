learning_rate: 0.001
adv_learning_rate: 0.001
weight_decay: 0.01
adv_weight_decay: 0.01
adv_grad_factor: 1.0
activation: relu
use_batch_norm: true
dropout: 0.2
n_common_layers: 3
n_common_units: 10
n_adv_layers: 3
n_adv_units: 10
